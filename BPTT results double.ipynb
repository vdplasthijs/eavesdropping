{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import BPTTRNN as bp\n",
    "import time\n",
    "import seaborn as sns\n",
    "# import sklearn.svm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create synthetic data\n",
    "There are 8 possible values: \n",
    "- 0 $blank/0$\n",
    "- 1 $A_1$\n",
    "- 2 $A_2$\n",
    "- 3 $B_1$\n",
    "- 4 $B_2$\n",
    "- 5 $C_1$\n",
    "- 6 $C_2$\n",
    "- 7 $D$\n",
    "\n",
    "Data points are thus 8-dim vectors, with:\n",
    "\n",
    "$z_{k, t} = (1_{0}, 1_{A_1}, 1_{A_2}, ... , 1_{D})$\n",
    "\n",
    "where $k$ is the trial index, and $t$ is the trial time. Hence $|z_{k, t}| = 1 $ before white noise is added. Trials will be $T=9$ data points long, and of form:\n",
    "\n",
    "$ 0, A_{\\alpha}, 0, B_{\\alpha}, 0, C_{\\beta}, 0, D, 0 $\n",
    "\n",
    "where $\\alpha, \\beta \\in (1, 2)$, although one can use shorter sequences for testing (e.g. $0, A_{\\alpha}, 0, B_{\\alpha}, 0$). The expected sequence is $\\alpha = \\beta$, and the unexpected sequence is $\\alpha \\neq \\beta$.\n",
    "\n",
    "Network input $x_k = z_{k, 0:T-1}$ and output $y_k = z_{k, 1:T}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total = 1000  # total number of data sequences\n",
    "n_freq = 8  # blank through D\n",
    "n_times = 9\n",
    "doublesse = True\n",
    "ratio_train = 0.8\n",
    "ratio_exp = 0.75  # probabilities of switching between alpha nd beta\n",
    "noise_scale = 0.01\n",
    "if doublesse:\n",
    "    eval_times = np.arange(4, 17)  # single\n",
    "elif doublesse is False:\n",
    "    eval_times = np.arange(1, 8)  # single\n",
    "\n",
    "freq_labels = ['0', 'A1', 'A2', 'B1', 'B2', 'C1', 'C2', 'D']\n",
    "tmp0, tmp1 = bp.generate_synt_data(n_total=n_total, \n",
    "                                   n_times=n_times, n_freq=n_freq,\n",
    "                                   ratio_train=ratio_train, ratio_exp=ratio_exp, \n",
    "                                   noise_scale=noise_scale, double_length=doublesse)\n",
    "x_train, y_train, x_test, y_test = tmp0\n",
    "labels_train, labels_test = tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb9cf8155f8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y_test[0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model with BPTT\n",
    "\n",
    "#### RNN model:\n",
    "\n",
    "$(x_t, s_{t-1}) \\to s_t \\to \\hat{y}_t == y_t = x_{t+1} $\n",
    "\n",
    "within one trial $k$. \n",
    "Equations:\n",
    "\n",
    "$s_t = \\tanh( U \\cdot x_t + W \\cdot s_{t-1})$\n",
    "\n",
    "\n",
    "$\\hat{y}_t = softmax(V \\cdot s_t) = \\left( \\frac{e^{V_i \\cdot s_t}}{\\sum_i e^{V_i \\cdot s_t}} \\right), \\; for \\; i \\in (0, A_1, A_2 ... D)$\n",
    "\n",
    "where $U_{n x f}, W_{n x n}, V_{f x n}$ are matrices where $n$ is the number of RNN nodes and $f$ the number of input/output frequencies.\n",
    "\n",
    "#### Training procedure:\n",
    "\n",
    "The full sequence $x_k$ is forwarded through the model, yielding $\\hat{y}_k$. This is used to compute the loss function $L$, which error is backpropagated through time (BPTT) to the parameters $\\theta = (U, W, V)$ (updated with SGD probably). \n",
    "The loss function $L$ uses cross entropy and L1 regularisation:\n",
    "\n",
    "$L_k = \\sum_{\\tau} - y_{k, \\tau} \\log \\hat{y}_{k, \\tau} + \\lambda \\cdot ||\\theta||_1$\n",
    "\n",
    "where $\\tau$ defines the trial times that are taken into account for Loss computation. \n",
    "\n",
    "$ \\begin{equation}\n",
    "    \\tau =\n",
    "    \\begin{cases}\n",
    "      (0, 1, 2, ... 8), & \\text{all} \\\\\n",
    "      (1, 3, 5, 7), & \\text{non-blank (nb)}\\\\\n",
    "      (3, 5, 7), & \\text{non-initial nb (ninb)}\n",
    "    \\end{cases}\n",
    "  \\end{equation}$\n",
    "\n",
    "#### Initial conditions:\n",
    "\n",
    "Tricky.. Maybe start on each trials with either zero, or small-magnitude noise, for $s_{k, -1}$? The sequence $0, A_{\\alpha}, 0$ then should provide a sensible initialisation, needed for $B_{\\alpha}$ prediction... ?  \n",
    "\n",
    "\n",
    "#### Pseudo-algorithm:\n",
    "\n",
    "    for $it$ in epochs:\n",
    "        for $k$ in trials:\n",
    "            for $t$ in times:\n",
    "                rnn.forward($x_t$)\n",
    "            compute loss $L_k$\n",
    "            update parameters with BPTT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set training parameters:\n",
    "n_nodes = 20  # number of nodes in the RNN \n",
    "if doublesse:\n",
    "    learning_rate = 0.01\n",
    "elif doublesse is False:\n",
    "    learning_rate = 0.05\n",
    "bs = 1  # batch size\n",
    "n_epochs = 1000\n",
    "l1_param = 5e-4  # L1 regularisation in loss function\n",
    "# check_conv = False\n",
    "# conv_rel_tol = 1e-6\n",
    "# prev_loss = 10  \n",
    "\n",
    "## Create data loader objects:\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)\n",
    "\n",
    "test_ds = TensorDataset(x_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initiate RNN model\n",
    "rnn = bp.RNN(n_stim=n_freq, n_nodes=n_nodes)  # Create RNN class\n",
    "opt = torch.optim.SGD(rnn.parameters(), lr=learning_rate)  # call optimiser from pytorhc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising training\n",
      "Train performance:\n"
     ]
    }
   ],
   "source": [
    "## Training procedure\n",
    "print('Initialising training')\n",
    "print('Train performance:')\n",
    "\n",
    "train_loss_arr = np.zeros(n_epochs)  # save loss during training \n",
    "test_loss_arr = np.zeros(n_epochs)\n",
    "for epoch in range(n_epochs):  # repeating epochs\n",
    "    rnn.train()  # set to train model (i.e. allow gradient computation/tracking)\n",
    "    for xb, yb in train_dl:  # returns torch(n_bs x n_times x n_freq)\n",
    "        full_pred = bp.compute_full_pred(model=rnn, xdata=xb)  # predict time trace\n",
    "        loss = bp.tau_loss(y_est=full_pred, y_true=yb, model=rnn, \n",
    "                           reg_param=l1_param, tau_array=eval_times)  # compute loss\n",
    "        loss.backward()  # compute gradients\n",
    "        opt.step()  # update \n",
    "        opt.zero_grad()   # reset \n",
    "\n",
    "    rnn.eval()  # evaluation mode -> disable gradient tracking\n",
    "    with torch.no_grad():  # to be sure\n",
    "        ## Compute losses for saving:\n",
    "        full_pred = bp.compute_full_pred(model=rnn, xdata=x_train)\n",
    "        loss = bp.tau_loss(y_est=full_pred, y_true=y_train, model=rnn, \n",
    "                           reg_param=l1_param, tau_array=eval_times)\n",
    "        train_loss_arr[epoch] = float(loss.detach().numpy())\n",
    "        if epoch % 10 == 1:\n",
    "            print(f'epoch {epoch},  loss: {loss}')\n",
    "      \n",
    "        full_pred = bp.compute_full_pred(model=rnn, xdata=x_test)\n",
    "        loss = bp.tau_loss(y_est=full_pred, y_true=y_test, model=rnn, \n",
    "                           reg_param=l1_param, tau_array=eval_times)\n",
    "        test_loss_arr[epoch] = float(loss.detach().numpy())\n",
    "        \n",
    "#     if check_conv:\n",
    "#         new_loss = loss_function(pred, yb).detach().numpy()\n",
    "#         diff = np.abs(new_loss - prev_loss) / (new_loss + prev_loss)\n",
    "#         if diff < conv_rel_tol:\n",
    "#             print(f'Converged at epoch {epoch},  loss: {loss_function(pred, yb)}')\n",
    "#             break\n",
    "#         prev_loss = new_loss\n",
    "rnn.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7, 4)\n",
    "plt.plot(train_loss_arr, label='train')\n",
    "plt.plot(test_loss_arr, label='test')\n",
    "plt.xlabel('Epoch'); plt.ylabel(\"Loss\"); plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_pred = bp.compute_full_pred(xdata=x_test,  model=rnn)\n",
    "print(bp.tau_loss(y_est=full_test_pred, y_true=y_test, model=rnn, reg_param=l1_param))   \n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(y_test.mean(0).numpy())\n",
    "plt.title(\"Mean y_test data\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(full_test_pred.mean(0).detach().numpy())\n",
    "plt.title(\"Mean predicted y data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "print(f'Trial type: {labels_test[k]}')\n",
    "pred = bp.compute_full_pred(x_test[k,:,:], model=rnn)\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "\n",
    "plt.subplot(121)\n",
    "sns.heatmap(y_test[k, :, :].numpy().T, yticklabels=freq_labels)\n",
    "plt.title(\"y test data\"); plt.xlabel('Time'); plt.ylabel('True data')\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.heatmap(pred.mean(0).detach().numpy().T, vmax=1, vmin=0, yticklabels=freq_labels)\n",
    "plt.title(\"predicted y data\"); plt.xlabel('Time'); plt.ylabel(\"Predictions\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (18, 5)\n",
    "\n",
    "def plot_weights(ax, rnn_layer, title='weights'):\n",
    "    weights = [x for x in rnn_layer.parameters()][0].detach().numpy().T\n",
    "    cutoff = np.percentile(np.abs(weights), 95)\n",
    "    sns.heatmap(weights, ax=ax, cmap='PiYG', vmax=cutoff, vmin=-1 * cutoff)\n",
    "    ax.set_title(title); \n",
    "    return ax\n",
    "\n",
    "fig, ax_w = plt.subplots(1, 3)\n",
    "plot_weights(ax=ax_w[0], rnn_layer=rnn.lin_input,\n",
    "            title='Input stimulus-neuron weights')\n",
    "ax_w[0].set_yticklabels(freq_labels)\n",
    "ax_w[0].set_xlabel('Neuron'); ax_w[1].set_ylabel('Stimulus')\n",
    "\n",
    "plot_weights(ax=ax_w[1], rnn_layer=rnn.lin_feedback,\n",
    "            title='Feedback neuron-neuron weights')\n",
    "ax_w[1].set_xlabel('Neuron'); ax_w[1].set_ylabel('Neuron')\n",
    "\n",
    "plot_weights(ax=ax_w[2], rnn_layer=rnn.lin_output,\n",
    "            title='Ouput neuron-prediction weights')\n",
    "ax_w[2].set_xticklabels(freq_labels)\n",
    "ax_w[2].set_ylabel('Neuron'); ax_w[2].set_xlabel('Output')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
