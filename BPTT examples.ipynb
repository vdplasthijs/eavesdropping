{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import seaborn as sns\n",
    "import time\n",
    "import sklearn.svm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from BPTTRNN import TBPTT, MyMod\n",
    "seq_len = 20\n",
    "layer_size = 50\n",
    "idx = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TBPTT():\n",
    "    def __init__(self, one_step_module, loss_module, k1, k2, optimizer):\n",
    "        self.one_step_module = one_step_module\n",
    "        self.loss_module = loss_module\n",
    "        self.k1 = k1\n",
    "        self.k2 = k2\n",
    "        self.retain_graph = k1 < k2\n",
    "        # You can also remove all the optimizer code here, and the\n",
    "        # train function will just accumulate all the gradients in\n",
    "        # one_step_module parameters\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train(self, input_sequence, init_state):\n",
    "        states = [(None, init_state)]\n",
    "        for j, (inp, target) in enumerate(input_sequence):\n",
    "\n",
    "            state = states[-1][1].detach()\n",
    "            state.requires_grad=True\n",
    "            output, new_state = self.one_step_module(inp, state)\n",
    "            states.append((state, new_state))\n",
    "\n",
    "            while len(states) > self.k2:\n",
    "                # Delete stuff that is too old\n",
    "                del states[0]\n",
    "\n",
    "            if (j+1)%self.k1 == 0:\n",
    "                loss = self.loss_module(output, target)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                # backprop last module (keep graph only if they ever overlap)\n",
    "                start = time.time()\n",
    "                loss.backward(retain_graph=self.retain_graph)\n",
    "                for i in range(self.k2-1):\n",
    "                    # if we get all the way back to the \"init_state\", stop\n",
    "                    if states[-i-2][0] is None:\n",
    "                        break\n",
    "                    curr_grad = states[-i-1][0].grad\n",
    "                    states[-i-2][1].backward(curr_grad, retain_graph=self.retain_graph)\n",
    "                print(\"bw: {}\".format(time.time()-start))\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyMod(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyMod, self).__init__()\n",
    "        self.lin = nn.Linear(2*layer_size, 2*layer_size)\n",
    "\n",
    "    def forward(self, inp, state):\n",
    "        global idx\n",
    "        full_out = self.lin(torch.cat([inp, state], 1))\n",
    "        # out, new_state = full_out.chunk(2, dim=1)\n",
    "        out = full_out.narrow(1, 0, layer_size)  # 0:layer_size \n",
    "        new_state = full_out.narrow(1, layer_size, layer_size)  # layer_size:(2 * layersize)\n",
    "        def get_pr(idx_val):\n",
    "            def pr(*args):\n",
    "                print(\"doing backward {}\".format(idx_val))\n",
    "            return pr\n",
    "        new_state.register_hook(get_pr(idx))\n",
    "        out.register_hook(get_pr(idx))\n",
    "        print(\"doing fw {}\".format(idx))\n",
    "        idx += 1\n",
    "        return out, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing fw 0\n",
      "doing fw 1\n",
      "doing fw 2\n",
      "doing fw 3\n",
      "doing fw 4\n",
      "doing backward 4\n",
      "doing backward 3\n",
      "doing backward 2\n",
      "doing backward 1\n",
      "doing backward 0\n",
      "bw: 0.01666116714477539\n",
      "doing fw 5\n",
      "doing fw 6\n",
      "doing fw 7\n",
      "doing fw 8\n",
      "doing fw 9\n",
      "doing backward 9\n",
      "doing backward 8\n",
      "doing backward 7\n",
      "doing backward 6\n",
      "doing backward 5\n",
      "doing backward 4\n",
      "doing backward 3\n",
      "bw: 0.01590895652770996\n",
      "doing fw 10\n",
      "doing fw 11\n",
      "doing fw 12\n",
      "doing fw 13\n",
      "doing fw 14\n",
      "doing backward 14\n",
      "doing backward 13\n",
      "doing backward 12\n",
      "doing backward 11\n",
      "doing backward 10\n",
      "doing backward 9\n",
      "doing backward 8\n",
      "bw: 0.008711099624633789\n",
      "doing fw 15\n",
      "doing fw 16\n",
      "doing fw 17\n",
      "doing fw 18\n",
      "doing fw 19\n",
      "doing backward 19\n",
      "doing backward 18\n",
      "doing backward 17\n",
      "doing backward 16\n",
      "doing backward 15\n",
      "doing backward 14\n",
      "doing backward 13\n",
      "bw: 0.004476070404052734\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "one_step_module = MyMod()\n",
    "loss_module = nn.MSELoss()\n",
    "input_sequence = [(torch.rand(200, layer_size), torch.rand(200, layer_size))] * seq_len\n",
    "\n",
    "optimizer = torch.optim.SGD(one_step_module.parameters(), lr=1e-3)\n",
    "\n",
    "runner = TBPTT(one_step_module, loss_module, 5, 7, optimizer)\n",
    "\n",
    "runner.train(input_sequence, torch.zeros(200, layer_size))\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
