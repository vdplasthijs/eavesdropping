{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import BPTTRNN as bp\n",
    "import time\n",
    "# import sklearn.svm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create synthetic data\n",
    "There are 8 possible values: \n",
    "- 0 $blank/0$\n",
    "- 1 $A_1$\n",
    "- 2 $A_2$\n",
    "- 3 $B_1$\n",
    "- 4 $B_2$\n",
    "- 5 $C_1$\n",
    "- 6 $C_2$\n",
    "- 7 $D$\n",
    "\n",
    "Data points are thus 8-dim vectors, with:\n",
    "\n",
    "$z_{k, t} = (1_{0}, 1_{A_1}, 1_{A_2}, ... , 1_{D})$\n",
    "\n",
    "where $k$ is the trial index, and $t$ is the trial time. Hence $|z_{k, t}| = 1 $ before white noise is added. Trials will be $T=9$ data points long, and of form:\n",
    "\n",
    "$ 0, A_{\\alpha}, 0, B_{\\alpha}, 0, C_{\\beta}, 0, D, 0 $\n",
    "\n",
    "where $\\alpha, \\beta \\in (1, 2)$, although I must use shorter sequences for testing (e.g. $0, A_{\\alpha}, 0, B_{\\alpha}, 0$).\n",
    "\n",
    "Network input $x_k = z_{k, 0:T-1}$ and output $y_k = z_{k, 1:T}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total = 100  # total number of data sequences\n",
    "n_freq = 3  # blank through D\n",
    "n_times = 5\n",
    "\n",
    "ratio_train = 0.8\n",
    "ratio_exp, ratio_unexp = 0.5, 0.5  # probabilities of switching between alpha nd beta\n",
    "noise_scale = 0.05\n",
    "\n",
    "n_train = int(ratio_train * n_total)\n",
    "n_test = n_total - n_train\n",
    "ratio_control = 0\n",
    "ratio_exp, ratio_unexp = ratio_exp / (ratio_exp + ratio_unexp + ratio_control),  ratio_unexp / (ratio_exp + ratio_unexp + ratio_control)\n",
    "assert ratio_exp + ratio_unexp + ratio_control == 1\n",
    "\n",
    "## simple data generation for now:\n",
    "all_seq = np.zeros((n_total, n_times, n_freq))\n",
    "all_seq[:, 0, 0] = 1\n",
    "all_seq[:, 1, 1] = 1\n",
    "all_seq[:, 2, 0] = 1\n",
    "all_seq[:, 3, 2] = 1\n",
    "all_seq[:, 4, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb08a08f7f0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(all_seq.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train/test data:\n",
    "\n",
    "shuffle_ind = np.random.choice(a=n_total, size=n_total, replace=False)\n",
    "all_seq = all_seq[shuffle_ind, :, :]  # shuffle randomly, (TODO: Stratified split)\n",
    "train_seq = all_seq[:n_train, :, :]\n",
    "test_seq = all_seq[n_train:, :, :]\n",
    "x_train = train_seq[:, :-1, :] + (np.random.randn(n_train, n_times - 1, n_freq) * noise_scale)\n",
    "y_train = train_seq[:, 1:, :]\n",
    "x_test = test_seq[:, :-1, :] + (np.random.randn(n_test, n_times - 1, n_freq) * noise_scale)\n",
    "y_test = test_seq[:, 1:, :]\n",
    "x_train, y_train, x_test, y_test = map(\n",
    "    torch.tensor, (x_train, y_train, x_test, y_test))\n",
    "x_train, y_train, x_test, y_test = x_train.float(), y_train.float(), x_test.float(), y_test.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model with BPTT\n",
    "\n",
    "#### RNN model:\n",
    "\n",
    "$(x_t, s_{t-1}) \\to s_t \\to \\hat{y}_t == y_t = x_{t+1} $\n",
    "\n",
    "within one trial $k$. \n",
    "Equations:\n",
    "\n",
    "$s_t = \\tanh( U \\cdot x_t + W \\cdot s_{t-1})$\n",
    "\n",
    "\n",
    "$\\hat{y}_t = softmax(V \\cdot s_t) = \\left( \\frac{e^{V_i \\cdot s_t}}{\\sum_i e^{V_i \\cdot s_t}} \\right), \\; for \\; i \\in (0, A_1, A_2 ... D)$\n",
    "\n",
    "where $U_{n x f}, W_{n x n}, V_{f x n}$ are matrices where $n$ is the number of RNN nodes and $f$ the number of input/output frequencies.\n",
    "\n",
    "#### Training procedure:\n",
    "\n",
    "The full sequence $x_k$ is forwarded through the model, yielding $\\hat{y}_k$. This is used to compute the loss function $L$, which error is backpropagated through time (BPTT) to the parameters $U, W, V$ (updated with SGD probably). \n",
    "The loss function $L$ uses cross entropy:\n",
    "\n",
    "$L_k = \\sum_{\\tau} - y_{k, \\tau} \\log \\hat{y}_{k, \\tau}$ + regularisation?\n",
    "\n",
    "where $\\tau$ defines the trial times that are taken into account for Loss computation. \n",
    "\n",
    "$ \\begin{equation}\n",
    "    \\tau =\n",
    "    \\begin{cases}\n",
    "      (0, 1, 2, ... 8), & \\text{all} \\\\\n",
    "      (1, 3, 5, 7), & \\text{non-blank (nb)}\\\\\n",
    "      (3, 5, 7), & \\text{non-initial nb (ninb)}\n",
    "    \\end{cases}\n",
    "  \\end{equation}$\n",
    "\n",
    "#### Initial conditions:\n",
    "\n",
    "Tricky.. Maybe start on each trials with either zero, or small-magnitude noise, for $s_{k, -1}$? The sequence $0, A_{\\alpha}, 0$ then should provide a sensible initialisation, needed for $B_{\\alpha}$ prediction... ?  \n",
    "\n",
    "\n",
    "#### Pseudo-algorithm:\n",
    "\n",
    "    for $it$ in epochs:\n",
    "        for $k$ in trials:\n",
    "            for $t$ in times:\n",
    "                rnn.forward($x_t$)\n",
    "            compute loss $L_k$\n",
    "            update parameters with BPTT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = 20\n",
    "learning_rate = 0.1\n",
    "bs = 1\n",
    "n_epochs = 100\n",
    "check_conv = False\n",
    "conv_rel_tol = 1e-6\n",
    "l1_param = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)\n",
    "\n",
    "test_ds = TensorDataset(x_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = bp.RNN(n_stim=n_freq, n_nodes=n_nodes)\n",
    "opt = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "prev_loss = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising traing\n",
      "Train performance:\n",
      "epoch 1,  loss: 0.06037118285894394\n",
      "epoch 11,  loss: 0.05953645706176758\n",
      "epoch 21,  loss: 0.05870130658149719\n",
      "epoch 31,  loss: 0.0579422190785408\n",
      "epoch 41,  loss: 0.05715874582529068\n",
      "epoch 51,  loss: 0.05644970387220383\n",
      "epoch 61,  loss: 0.05577225238084793\n",
      "epoch 71,  loss: 0.0551016591489315\n",
      "epoch 81,  loss: 0.054503027349710464\n",
      "epoch 91,  loss: 0.053896404802799225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (lin_input): Linear(in_features=3, out_features=20, bias=True)\n",
       "  (lin_feedback): Linear(in_features=20, out_features=20, bias=True)\n",
       "  (lin_output): Linear(in_features=20, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Initialising traing')\n",
    "print('Train performance:')\n",
    "\n",
    "loss_list = []\n",
    "for epoch in range(n_epochs):\n",
    "    rnn.train()\n",
    "    for xb, yb in train_dl:  # returns torch(n_bs x n_times x n_freq)\n",
    "        full_pred = bp.compute_full_pred(model=rnn, xdata=xb, ydata=yb)\n",
    "        loss = bp.tau_loss(y_est=full_pred, y_true=yb, model=rnn, reg_param=l1_param)\n",
    "        loss_list.append(float(loss.detach().numpy()))\n",
    "        loss.backward()  # compute gradients\n",
    "        opt.step()  # update \n",
    "        opt.zero_grad()  # reset\n",
    "        \n",
    "    rnn.eval()\n",
    "    if epoch % 10 == 1:\n",
    "        print(f'epoch {epoch},  loss: {bp.tau_loss(y_est=full_pred, y_true=yb, model=rnn)}')\n",
    "        \n",
    "#     if check_conv:\n",
    "#         new_loss = loss_function(pred, yb).detach().numpy()\n",
    "#         diff = np.abs(new_loss - prev_loss) / (new_loss + prev_loss)\n",
    "#         if diff < conv_rel_tol:\n",
    "#             print(f'Converged at epoch {epoch},  loss: {loss_function(pred, yb)}')\n",
    "#             break\n",
    "#         prev_loss = new_loss\n",
    "rnn.eval()\n",
    "# with torch.no_grad():\n",
    "#     print('Test performance:')\n",
    "#     for xb, yb in test_dl:\n",
    "#         test_pred = bp.compute_full_pred(xdata=xb, ydata=yb, model=rnn)\n",
    "#         print(bp.tau_loss(y_est=test_pred, y_true=y_test, model=rnn, reg_param=l1_param))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0535, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp.tau_loss(y_est=full_pred, y_true=yb, model=rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape xdata: torch.Size([20, 4, 3])\n",
      "n samples: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0056, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = bp.compute_full_pred(xdata=x_test, ydata=y_test, model=rnn)\n",
    "bp.tau_loss(y_est=test_pred, y_true=y_test, model=rnn, reg_param=l1_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test performance:\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0057)\n",
      "tensor(0.0056)\n",
      "tensor(0.0055)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0057)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0055)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0057)\n",
      "tensor(0.0056)\n",
      "tensor(0.0055)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0057)\n",
      "tensor(0.0056)\n",
      "tensor(0.0057)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0057)\n",
      "tensor(0.0056)\n",
      "tensor(0.0057)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0057)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0057)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0057)\n",
      "tensor(0.0056)\n",
      "tensor(0.0057)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0057)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0056)\n",
      "tensor(0.0057)\n",
      "tensor(0.0057)\n",
      "tensor(0.0057)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('Test performance:')\n",
    "    for xb, yb in train_dl:\n",
    "        test_pred = bp.compute_full_pred(xdata=xb, ydata=yb, model=rnn)\n",
    "        print(bp.tau_loss(y_est=test_pred, y_true=yb, model=rnn, reg_param=l1_param))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bp.tau_loss(y_est=test_pred, y_true=y_test, model=rnn, reg_param=l1_param))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
